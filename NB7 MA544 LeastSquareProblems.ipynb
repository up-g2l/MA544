{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Least Square Problems\n",
    "---\n",
    "**Objectives and Plan**\n",
    "\n",
    "1. Overdetermined Systems\n",
    "1. Least Square Problems Formulations\n",
    "1. Normal Equations\n",
    "1. Solving full-rank LSP by various Methods\n",
    "1. Solving rank-deficient LSP by SVD\n",
    "1. Variations of LSP: Weighted, generalized and regularized LSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#IMPORT\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.image as mpimg\n",
    "#%matplotlib inline\n",
    "# Equation Box \\large \\bbox[20px, #90caf9, border: 1px solid gray]{}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Overdetermined Linear System of Equations\n",
    "---\n",
    "Consider the following system of $m$ linear equations in $n$ variables.\n",
    "\\begin{align}\n",
    "a_{11} x_1 + a_{12} x_2  + \\cdots + a_{1n} x_n  &= b_1 \\\\\n",
    "a_{21} x_1 + a_{22} x_2  + \\cdots + a_{2n} x_n  &= b_2 \\\\\n",
    " \\vdots \\qquad \\qquad   & \\ \\\\\n",
    "a_{m1} x_1 + a_{m2} x_2  + \\cdots + a_{mn} x_n  &= b_ m,\n",
    "\\end{align}\n",
    "\n",
    "-  The solution of a linear system represents the **point of intersection of hyperplanes** given by the lienar equations.\n",
    "\n",
    "\n",
    "-  The solution also represents the **linear coding of the columns** of a matrix $A$ to get a vector $b$ in the column space ($\\mathcal{C}(A)$).\n",
    " \n",
    "$$\n",
    " x_1 \\begin{pmatrix}a_{11}\\\\a_{21}\\\\ \\vdots \\\\a_{m1}\\end{pmatrix} +\n",
    " x_2 \\begin{pmatrix}a_{12}\\\\a_{22}\\\\ \\vdots \\\\a_{m2}\\end{pmatrix} +\n",
    " \\cdots +\n",
    " x_n \\begin{pmatrix}a_{1n}\\\\a_{2n}\\\\ \\vdots \\\\a_{mn}\\end{pmatrix}\n",
    " =\n",
    " \\begin{pmatrix}b_1\\\\b_2\\\\ \\vdots \\\\b_m\\end{pmatrix}\n",
    " $$\n",
    " \n",
    " or simply as\n",
    " $$\n",
    " x_1 A_{:1}+x_2 A_{:2}+ \\cdots +x_n A_{:n} = \\mathbf{b}\n",
    " $$\n",
    " \n",
    "\n",
    "- The system could be represented in a compact form as $A\\mathbf{x} = b$, where \n",
    " $$\n",
    " A=\n",
    "\\begin{pmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn}\n",
    "\\end{pmatrix},\\quad\n",
    "\\mathbf{x}=\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{pmatrix},\\quad\n",
    "\\mathbf{b}=\n",
    "\\begin{pmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_m\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The system is called...\n",
    "  >- **Square system** when $m = n$. Admits a unique solution only when $A$ is invertible.\n",
    "     \n",
    "  >- **overdetermined** when $m > n$.\n",
    "  \n",
    "  >- **underdetermined** when $ m < n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Pick up all the correct statements\n",
    "\n",
    "   A. Commonly, an over-determined system does not have any solutions.\n",
    "   \n",
    "   B. No over-determined system has any solution.\n",
    "   \n",
    "   C. An underdetermined system has infinite many solutions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Square Problems (LSP)\n",
    "When $m > n$ the number of variables is less than the number of equations. These systems are rarely exactly solvable. However, in an approximate sense, we would like to find a vector $A\\mathbf{x}$ in the column space of $A$ which is closest to $\\mathbf{b}$. In such a case, we aim to find $\\mathbf{x}$ such that the residual vector $\\mathbf{r}=\\mathbf{b}-A\\mathbf{x}$ as small as possible.\n",
    "<img src=\"https://live.staticflickr.com/65535/52739576429_a0be07aed5_z.jpg\" width=\"220\" height=\"214\" alt=\"LSProj\" align=\"right\" />\n",
    "<div class=\"alert alert-success\"> \n",
    "    \n",
    "<strong> LSP Formulation </strong>\n",
    "    \n",
    "For $A  \\in \\mathbb{R}^{m \\times n}$ (where $m \\geq n$) and $\\mathbf{b} \\in \\mathbb{R}^m$, find the vector $\\mathbf{x} \\in \\mathbb{R}^n$ that minimize\n",
    "$$\n",
    "    \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{\\operatorname{minimize}}  \\|A \\mathbf{x} - \\mathbf{b}\\|^2_2\n",
    "$$\n",
    "</div>    \n",
    "<span style='font-size:50px;'>&#129300;</span> Why should we use $l_2$ norm and not some other norms such as $l_1$ or $l_{\\infty}$ norm?\n",
    "\n",
    "<button data-toggle=\"collapse\" data-target=\"#demo\">Show why</button>\n",
    "\n",
    "<div id=\"demo\" class=\"collapse\">\n",
    "The function is differentiable in $l_2$ norm. Moreover, orthogonal transformations could be used as $l_2$ norms are invariant under multiplication of the vectors by orthogonal matrices. This leads to better numerical solutions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note the equivalent formulation of the minimizer as\n",
    "\n",
    "$$\\large \\bbox[10px, #90caf9, border: 1px solid gray]{\n",
    "\\hat{\\mathbf{x}} = \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{\\operatorname{argmin}} (\\mathbf{b}-A\\mathbf{x})^T(\\mathbf{b}-A\\mathbf{x})\n",
    "}$$\n",
    "\n",
    "\n",
    "<span style='font-size:50px;'>&#9971;</span> Note that from our discussion on matrix calculus and optimization, the gradient of the above function was found to be $A^T(A\\mathbf{x}-\\mathbf{b})$ which also leads to the normal equations discussed ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "<strong>Theorem</strong>\n",
    "<p>The vector $\\mathbf{x} \\in \\mathbb{R}^n$ minimizes the residual norm, $\\| \\mathbf{r} \\| = \\|\\mathbf{b} - A \\mathbf{x}\\|_2$, if and only if $\\mathbf{r} \\perp \\text{col}(A)$.\n",
    " </p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:inline;float:left;\" width=\"60%\">\n",
    "It follows from the above theorem that\n",
    "$$ \\mathbf{r} \\perp col(A) \\implies \\mathbf{r} \\in N(A^T) $$ as column space and left-null space are orthogonal.( Recall $\\mathcal{C}(A)\\oplus \\mathcal{N}(A^T)=\\mathbb{R}^m$. )\n",
    "\n",
    "This implies that \n",
    "\n",
    "\\begin{align*}\n",
    "\t&A^{T}\\mathbf{r} = 0 \\\\\n",
    "\t&A^{T}(\\mathbf{b}-A\\mathbf{x}) = 0 \\\\\n",
    "\t&A^{T}A\\mathbf{x} = A^{T}\\mathbf{b}\n",
    "\\end{align*}\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which naturally leads us to the **normal equations**\n",
    "$$\\large \\bbox[15px, #90caf9, border: 1px solid gray]{\n",
    "A^TA \\mathbf{x} = A^T \\mathbf{b}.\n",
    "}\n",
    "$$\n",
    "Note, the computation above attempts to demonstrate how the normal equations solve the LSP problem by minimizing the residual norm.\n",
    "\n",
    "**Discussion** The condition number while solving the normal equations.\n",
    "\n",
    "The **condition number** of rectangular matrix $B$ of rank $r$ could be given by\n",
    "$$\\large \\bbox[10px, #90caf9, border: 1px solid gray]{\n",
    "\\kappa(B) = \\frac{\\sigma_1(B)}{\\sigma_r(B)}\n",
    "}\n",
    "$$\n",
    "\n",
    "The condition number for the systems of normal eqaution thus comes out to be $\\kappa(A^TA) = \\frac{\\sigma_1^2}{\\sigma_r^2} = [\\kappa(A)]^2$\n",
    "\n",
    "<span style='font-size:50px;'>&#10071;</span> High condition number for $A$ suggest trouble in the case of nearly rank-deficient problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "<strong>Theorem</strong>\n",
    "    <p>For $A  \\in \\mathbb{R}^{m \\times n}$  (where $m \\geq n$)  and $\\mathbf{b} \\in \\mathbb{R}^m$, if the vector $\\mathbf{x} \\in \\mathbb{R}^n$ satisfies the condition $A^{T}A\\mathbf{x} = A^{T}\\mathbf{b}$, then $\\mathbf{x}$ solves the least square problem.\n",
    "        </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Solve the following system in the least square sense\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "3x+2y &=3\\\\\n",
    "2x+3y &=0\\\\\n",
    "x+2y &=1\n",
    "\\end{align}\n",
    "$$\n",
    "Or, in matrix-vector form\n",
    "$$\n",
    " \\underbrace{\\begin{pmatrix} 3 & 2 \\\\ 2 & 3  \\\\1 & 2 \\end{pmatrix}}_{A}\n",
    "\\underbrace{\\begin{pmatrix} x \\\\ y  \\end{pmatrix}}_{\\mathbf{x}} = \\underbrace{\\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix}}_{\\mathbf{b}}\n",
    "$$\n",
    "We have the normal equations $A^{T}A\\mathbf{x} = A^{T}\\mathbf{b}$\n",
    "$$\n",
    "\\begin{pmatrix} 14 & 14\\\\ 14 & 17 \\end{pmatrix}\\mathbf{x} = \\begin{pmatrix} 10\\\\8\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "By using Gaussian elimination, we have\n",
    "$$\n",
    "\\begin{pmatrix} 14 & 14 & \\vdots & 10\\\\ 14 & 17 & \\vdots & 8\\end{pmatrix}\\longrightarrow \\begin{pmatrix} 14 & 14 & \\vdots & 10\\\\ 0 & 3 & \\vdots & -2\\end{pmatrix}\n",
    "$$\n",
    "The last row from above gives $3x_2=-2 \\Rightarrow x_2=-\\frac{2}{3}$. \n",
    "\n",
    "From the first row we get $x_1(10-14(-2/3))/14=\\frac{29}{21}$.\n",
    "\n",
    "**Homework** Find the vector $\\mathbf{x}=(x_1,\\, x_2)^T$ that solves the following system in the least square sense\n",
    "$$\n",
    " \\begin{pmatrix} 1 & 2 \\\\ 1 & -1  \\\\1 & 3 \\end{pmatrix}\n",
    "\\begin{pmatrix} x_1 \\\\ x_2  \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\end{pmatrix}\n",
    "$$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Rank LSP Solutions\n",
    "\n",
    "When $A$ is full column rank, the matrix $A^TA$ is invertible and the normal equations coudl be solve simply (or naively) by\n",
    "$$\n",
    "A^{T}A\\mathbf{x} = A^{T}\\mathbf{b} \\iff \\mathbf{x} = (A^{T}A)^{-1}A^{T}\\mathbf{b}\n",
    "$$\n",
    "\n",
    "### Solving LSP by using Cholesky Decomposition\n",
    "It follows that $A^{T}A$ is a symmetric positive semi-definite matrix. Therefore, once our normal matrix has been constructed, we can apply Cholesky decomposition to further simplify our least square problem. In fact, the use of Cholesky decomposition allows the original problem to be stated in a way that is efficiently solved by numerical solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "<strong>Theorem: Cholesky Decomposition</strong>\n",
    "    \n",
    "For any Hermitian positive-definite matrix  $B \\in \\mathbb{R}^{n \\times n}$, there exists a qnique lower triangular matrix $L\\in \\mathbb{R}^{n \\times n}$ so that $B$ is decomposed as $B = LL^{T}$.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In LSP, if the matric $A$ is full column-rank, $A^{T}A$ is symmetric positive defintie and satisfies the conditions for Cholesky decomposition $A^{T}A = LL^{T}$. It follows that \n",
    "$$\n",
    "A^{T}A \\mathbf{x} = A^T \\mathbf{b} \\implies LL^{T}\\mathbf{x} = A^{T}\\mathbf{b}\n",
    "$$\n",
    "\n",
    "> By letting $L^{T}\\mathbf{x} = \\mathbf{z}$, we can solve the following system by forward substitution\n",
    "$$L\\mathbf{z} = A^{T}\\mathbf{b}$$\n",
    "\n",
    "> Once we have solved for $\\mathbf{z}$, we can finally solve for $\\mathbf{x}$ by backward substitution\n",
    "$$L^{T}\\mathbf{x} = \\mathbf{z}$$\n",
    "\n",
    " Note: The order of floating point operations is \n",
    "$$O(\\frac{1}{3} n^3 + mn^2) \\approx mn^2 + \\frac{1}{3}n^3 flops$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Solving LSP using QR Decomposition\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "<strong>Theorem: QR Decomposition</strong>\n",
    "  <p>For any matrix  $A \\in \\mathbb{R}^{m \\times n}$, there exists an orthogonal  matrix $Q\\in \\mathbb{R}^{m \\times m}$ and an upper triangular matrix $R \\in \\mathbb{R}^{m \\times n}$ so that $A$ is decomposed as $A = QR$.  </p>  \n",
    "  </div>\n",
    "  <img src=\"https://live.staticflickr.com/65535/52739811873_72506b7e37_z.jpg\" width=\"60%\"/>\n",
    "  \n",
    "<strong>Thin (Economy) QR </strong>\n",
    "For any matrix  $A \\in \\mathbb{R}^{m \\times n}$,\n",
    "$$\n",
    "A = Q_1 R_1\n",
    "$$\n",
    "where matrix $Q_1\\in \\mathbb{R}^{m \\times n}$ is an orthonormal column matrix and  $R \\in \\mathbb{R}^{n \\times n}$ is an upper triangular matrix.\n",
    "\n",
    "[Images Source](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTg3iy6aPjY93XaEev-6wox0rTsPpsT3A9PCw&usqp=CAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our original goal for least square problems\n",
    " $$\n",
    "    \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{\\operatorname{minimize}}  \\|A \\mathbf{x} - \\mathbf{b}\\|^2_2\n",
    " $$\n",
    "\n",
    "If we apply QR decomposition to $A$, we observe:\n",
    "$$\n",
    "\\|A \\mathbf{x}  - \\mathbf{b}\\|^2_2 \n",
    "= \\|QR \\mathbf{x}  - \\mathbf{b}\\|^2_2\n",
    "$$\n",
    "\n",
    "Since $Q$ is orthogonal, we can multiply by $Q^{T}$ and the 2-norm will remain unchanged. \n",
    "$$\n",
    "= \\|Q^{T}(QR \\mathbf{x} - \\mathbf{b})\\|^2_2\n",
    "=\\|Q^{T}QR \\mathbf{x}  - Q^{T}\\mathbf{b}\\|^2_2\\\\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which can be simplifed by using the thin-QR decomposition as follows\n",
    "\n",
    "$$\n",
    "\t \\|R \\mathbf{x}  - Q^{T}\\mathbf{b}\\|^2_2 \n",
    "     = \\left\\|\\begin{bmatrix} R_1\\\\ 0\\end{bmatrix} \\mathbf{x}  - \\begin{bmatrix} Q_1^T \\\\ Q_2^T\\\\ \\end{bmatrix} \\mathbf{b}\\right\\|^2_2\n",
    "    = \\left\\|\\begin{bmatrix} R_1 \\mathbf{x} -Q_1^T \\mathbf{b}\\\\ 0 - Q_2^T\\mathbf{b}\\end{bmatrix} \\right\\|^2_2 =  \\|R_1 \\mathbf{x}  - Q_1^{T}\\mathbf{b}\\|^2_2 + \\|Q_2^{T}\\mathbf{b}\\|_2^2\n",
    "$$\n",
    "\n",
    "<br>\n",
    "Finally, the expression on the right could be minimized by having $R_1 \\mathbf{x}  - Q_1^{T}\\mathbf{b}=\\mathbf{0}$. Since $R_1$ is upper triangular, we can solve for $ \\mathbf{x}$ by back-substitution $R_1 \\mathbf{x}  = Q_1^{T}\\mathbf{b}$. \n",
    "\n",
    "The order of floating point operations in this procedure is\n",
    "\n",
    "$$ 2mn^2 - \\frac{2}{3}n^3\\quad \\text{flops}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection with Projection\n",
    "\n",
    "When the solution of the LSP (in full column rank case) is $\\hat{\\mathbf{x}}=(A^TA)^{-1}A^T\\mathbf{b}$, the projection of vector $\\mathbf{b}$ onto the column space of $A$ is $A \\hat{\\mathbf{x}} = A(A^TA)^{-1}A^T\\mathbf{b}$. So, the projection matrix that projects vector $\\mathbf{b}$ onto the column space of $A$ is given by\n",
    "$$\n",
    "\\hat{H} = A(A^TA)^{-1}A^T,\n",
    "$$\n",
    "and the residual, $\\hat{\\mathbf{r}}=\\mathbf{b}-A \\hat{\\mathbf{x}} = (I-\\hat{H})\\mathbf{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving LSP by using SVD\n",
    "\n",
    "In the case of a full column-rank matrix $A$, the economy SVD could be given by $A=U_{(n)}\\Sigma_{(n)} V^T$, where $U_{(n)}$ is made of the first $n$ columns of $U$ (from full SVD). Please note that the dimensions of these matrices are $m \\times n$, $n \\times n$ and $n \\times n$ respectively. \n",
    "\n",
    "The projection matrix that projects $A$ onto the column space of $A$ could be given by $P=U_{(n)}U_{(n)}^T$ as the columns of $U_{(n)}$ provide an orthonormal basis for the column space of $A$. There for $\\mathbf{x}$ is such that $A\\mathbf{x}=P\\mathbf{b}$. This implies\n",
    "<br>\n",
    "$$\n",
    "U_{(n)}\\Sigma_{(n)} V^T \\mathbf{x}=U_{(n)}U_{(n)}^T \\mathbf{b} \\iff U^T_{(n)}U_{(n)}\\Sigma_{(n)} V^T \\mathbf{x}=U^T_{(n)}U_{(n)}U_{(n)}^T \\mathbf{b}\n",
    "$$\n",
    "<br>\n",
    "It should be noted that $U^T_{(n)}U_{(n)}=I_n$, so we have the equivalent system \n",
    "\n",
    "$$\n",
    "\\Sigma_{(n)} V^T \\mathbf{x}=U_{(n)}^T \\mathbf{b} \\implies  \\mathbf{x}= V \\Sigma_{(n)}^{-1} U_{(n)}^T \\mathbf{b} \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The number of floating point operations in this approach is \n",
    "$$\n",
    "\\approx 2 m n^2+11n^3\\quad  \\text{flops}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD and Rank-deficient (multi-collinear) LSP\n",
    "---\n",
    "When the matrix $A$ in the LSP is not full-rank, the normal equations have infinitely many solutions. Let $\\chi$ be the set of all such solutions\n",
    "$$\n",
    "\\chi = \\{ \\mathbf{x} \\in \\mathbf{R}^{n}: A^TA\\mathbf{x}=A^T\\mathbf{b}\\}\n",
    "$$\n",
    "\n",
    "<span style='font-size:50px;'>&#129300;</span> Can you prove that the set of all such least square solution is a convex set?\n",
    "\n",
    "Our aim is to find the unique LSP solution that has the smallest $l_2$ norm. We will denote it by $\\mathbf{x}_{LS}$. SVD provides the approach to find such a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"> \n",
    "<strong>Theorem</strong>\n",
    "    \n",
    "Let the SVD of rank $r$ matrix $A \\in \\mathbb{R}^{m\\times n}$, be given by \n",
    "    $$\n",
    "    A = \\sum_{i=1}^r \\sigma_i U_i V_i^T,\n",
    "    $$\n",
    "    in the outer product form, then the `smallest' least square solution to the LSP is given by \n",
    "    $$\n",
    "    \\mathbf{x}_{LS} = \\sum_{i=1}^r \\left( \\frac{U_i^T \\mathbf{b}}{\\sigma_i} \\right) V_i\n",
    "     $$\n",
    "     and the minimum residual norm is obtained by\n",
    "     $$\n",
    "     \\rho_{LS}^2 = \\sum_{i=r+1}^m (U_i^T \\mathbf{b})^2.\n",
    "     $$\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proof of the above follows from the fact that\n",
    "<br>\n",
    "$$\n",
    "\\|A\\mathbf{x}-\\mathbf{b}\\|_2^2=\\|U \\Sigma V^T \\mathbf{x}-\\mathbf{b}\\|_2^2=\\|\\Sigma V^T \\mathbf{x}-U^T\\mathbf{b}\\|_2^2=\\|\\Sigma  \\mathbf{z}-U^T\\mathbf{b}\\|_2^2\n",
    "$$\n",
    "<br>\n",
    "where $\\mathbf{z}=V^T\\mathbf{x}$. Then the expression on the right could be expressed as\n",
    "<br>\n",
    "$$\n",
    "\\left\\|\\ \n",
    "\\begin{pmatrix} \\sigma_1 z_1\\\\ \\vdots \\\\ \\sigma_r z_r \\\\0 \\\\  \\vdots \\\\ 0\\end{pmatrix}-\\begin{pmatrix} U_1^T \\mathbf{b}\\\\ \\vdots \\\\ U_r^T \\mathbf{b} \\\\U_{r+1}^T \\mathbf{b} \\\\  \\vdots \\\\ U_m^T \\mathbf{b}\\end{pmatrix}\\  \\right\\|_2^2=\n",
    "\\left\\|\\ \n",
    "\\begin{pmatrix} \\sigma_1 z_1-U_1^T \\mathbf{b}\\\\ \\vdots \\\\ \\sigma_r z_r-U_r^T \\mathbf{b} \\\\-U_{r+1}^T \\mathbf{b} \\\\  \\vdots \\\\ -U_m^T \\mathbf{b}\\end{pmatrix}\\  \\right\\|_2^2\n",
    "=\\sum_{i=1}^r (\\sigma_i z_i - U_i^T\\mathbf{b})^2+\\sum_{i=r+1}^m ( U_i^T\\mathbf{b})^2\n",
    "$$\n",
    "<br>\n",
    "Clearly, the above expression will admit the minimum value $\\sum_{i=r+1}^m ( U_i^T\\mathbf{b})^2$ only when the first expression is zero, implying that $z_i = U_i^T \\mathbf{b} /\\sigma_i$ for $i=1:r$. In addition, the smallest $\\|x\\|_2$ norm will require $z_i=0$ for $i=r+1:n$. Therefore,\n",
    "<br>\n",
    "$$\n",
    "\\mathbf{x}=V\\mathbf{z} = \\sum_{i=1}^r z_i V_i =\\sum_{i=1}^r  \\left( \\frac{U_i^T \\mathbf{b}}{\\sigma_i} \\right) V_i \n",
    "$$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudoinverse and LSP\n",
    "\n",
    "We defined pseudo-inverse when we studied SVD. Here is the definition again for all to recall.\n",
    "\n",
    "\n",
    "#### Pseudo-inverse (Moore-Penrose Inverse)\n",
    "\n",
    "The pseudo-inverse of a rectangular matrix $A \\in \\mathbb{R}^{m \\times n}$ of rank $r$, whose SVD is given by $A = U \\Sigma V^T$, is a unique rectangular matrix $A^+ \\in \\mathbb{R}^{n \\times m}$, given by\n",
    "\n",
    "$$\\large \n",
    "\\bbox[20px, #90caf9, border: 1px solid gray]{\n",
    "A^+ = V \\Sigma^+ U^T\n",
    "}\n",
    "$$\n",
    "where\n",
    "$$\\large\n",
    "\\Sigma^+ = \\textrm{diag}\\left(\\frac{1}{\\sigma_1},\\ \\frac{1}{\\sigma_2},\\ \\cdots,\\ \\frac{1}{\\sigma_r},\\ 0,\\ \\cdots,\\ 0\\right) \\in \\mathbb{R}^{n \\times m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> \n",
    "    \n",
    "It turns out from above that the calculation of smallest least square solution in the case of rank-deficient LSP could actaully be performed by the pseudo-inverse as \n",
    "    \n",
    "<br>\n",
    "    \n",
    "$$\n",
    "\\mathbf{x}_{LS} = A^+\\mathbf{b}\\quad \\text{and}\\quad \\rho_{LS} = \\|\\mathbf{b}-A \\mathbf{x}_{LS}\\|_2 = \\|\\mathbf{b}-A A^+\\mathbf{b}\\|_2=\\|(I-A A^+)\\mathbf{b}\\|_2\n",
    "$$\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have at different occassions proved several properties of pseudo-inverse. Here are some of them\n",
    "\n",
    "- When $m=n=rank(A)$, the pseudoinverse is the inverse: $A^+ = A^{-1}$.\n",
    "\n",
    "- When $A$ is full row-rank, $m=rank(A)$ and $A^+ = A^T(AA^T)^{-1}$ is the right inverse of $A$.\n",
    "\n",
    "- When  $A$ is full column-rank, $n=rank(A)$, $A^+ = (A^TA)^{-1}A^T$ is the left inverse of $A$.\n",
    "\n",
    "- The matrix $P=AA^+$ is an orthogonal projection onto column space $\\mathcal{C}(A)$. \n",
    "\n",
    "- The matrix $P=A^+A$ is an orthogonal projection onto row space $\\mathcal{R}(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   > Here is a problem that you should be able to solve by using the ideas from matrix calculus and optimization.\n",
    "<br>\n",
    "\n",
    "<span style='font-size:50px;'>&#129300;</span> \n",
    "The pseudoinverse is the unique minimal solution to the problem \n",
    "$$\n",
    "\\underset{X\\in \\mathbb{R}^{n \\times m}}{\\operatorname{minimize}} \\|AX-I\\|_F.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants for LSP\n",
    "\n",
    "<strong> Weighted Least Square Problems</strong>\n",
    "\n",
    "$$\\large \\bbox[10px, #90caf9, border: 1px solid gray]{\n",
    "\\hat{\\mathbf{x}} = \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{\\operatorname{argmin}} (\\mathbf{b}-A\\mathbf{x})^T W(\\mathbf{b}-A\\mathbf{x})\n",
    "}$$\n",
    "where $W$ is a  diagonal matrix of non-negative weights that assigns  weights to different observations in the least square formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Generalized Least Square Problems</strong>\n",
    "\n",
    "$$\\large \\bbox[10px, #90caf9, border: 1px solid gray]{\n",
    "\\hat{\\mathbf{x}} = \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{\\operatorname{argmin}} (\\mathbf{b}-A\\mathbf{x})^T \\Omega^{-1}(\\mathbf{b}-A\\mathbf{x})\n",
    "}$$\n",
    "where $\\Omega$ appears in the  decomposition (possibly Cholesky) of the covariance matrix $S$ as $S=\\Omega \\Omega^T$.\n",
    "\n",
    "<br>\n",
    "<span style='font-size:50px;'>&#129300;</span> Can you show that the formulation of the generalized LSP is equivalent to the follwing constrained problem.\n",
    "$$\n",
    "\\text{minimize} \\ \\mathbf{v}^T\\mathbf{v} \\quad \\text{subject to }\\quad \\mathbf{b}=A\\mathbf{x}+\\Omega \\mathbf{v}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong> Column-weighted Least Square Problems</strong>\n",
    "If we scale the columns of the matrix $A$ then the LSP computes the solution of\n",
    "$$\n",
    "    \\underset{\\mathbf{z} \\in \\mathbb{R}^n}{\\operatorname{minimize}}  \\|(AG^{-1}) \\mathbf{z} - \\mathbf{b}\\|^2_2\n",
    "$$\n",
    "\n",
    "<br>\n",
    "Such scaling is common place, for example, in normalization of features of a data-matrix such as\n",
    "$$\n",
    "G = G_2:=\\text{diag}\\{ \\|A_1\\|_2,\\, \\|A_2\\|_2,\\, \\cdots,\\,\\|A_n\\|_2 \\}\n",
    "$$\n",
    "It has been shown that with this choice of the scaling, the condition number $\\kappa(AG^{-1})$ is approximately minimized that affects the accuracy of $\\mathbf{z}_{LS}$. However, some orthogonalization based methods may not return the same estimate (see p.307 Golub)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized LSP\n",
    "\n",
    "In <strong>ridge regression</strong>, we solve an optimization problem in the following form\n",
    "$$\n",
    "    \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{\\operatorname{minimize}}  \\|A \\mathbf{x} - \\mathbf{b}\\|^2_2 + \\lambda \\| \\mathbf{x} \\|^2_2\n",
    "$$\n",
    "where the ridge-penalty $\\lambda$ is chosen to shape the solution in some desired way.\n",
    "<br>\n",
    "\n",
    "<span style='font-size:50px;'>&#129300;</span> Can you show that the normal equations for this problem is given by \n",
    "$$\n",
    "(A^TA+\\lambda I)\\mathbf{x} = A^T \\mathbf{b}\n",
    "$$\n",
    "\n",
    "One clear advantage of using this regularization is that it takes care of the near rank-deficiency (multicolliniarity) of $A$ as $A^TA+\\lambda I$ could be made invertible by choosing $\\lambda$ appropriately. See Golub (p 307-308) on how $\\lambda$ could be chosen in an optimum way.\n",
    "\n",
    "<br>\n",
    "<span style='font-size:50px;'>&#129300;</span> Follow the SVD of $A=U \\Sigma V^T$ method discussed earlier to show that\n",
    "$$\n",
    "(\\Sigma^T \\Sigma+\\lambda I) V^T \\mathbf{x} = \\Sigma^T U^T \\mathbf{b}\n",
    "$$\n",
    "and hence\n",
    "$$\n",
    "\\mathbf{x}(\\lambda) = \\sum_{i=1}^{r} \\frac{\\sigma_i U_i^T \\mathbf{b}}{\\sigma_i^2 +\\lambda} V_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In <strong>Tikhonov regularization problem</strong>, we solve an optimization problem in the  form\n",
    "$$\n",
    "    \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{\\operatorname{minimize}}  \\|A \\mathbf{x} - \\mathbf{b}\\|^2_2 + \\lambda \\| B \\mathbf{x} \\|^2_2\n",
    "$$\n",
    "where $B \\in \\mathbb{R}^{n \\times n}$.\n",
    "<br>\n",
    "\n",
    "<span style='font-size:50px;'>&#129300;</span> Can you show that the normal equations for this problem is given by \n",
    "$$\n",
    "(A^TA+\\lambda B^TB)\\mathbf{x} = A^T \\mathbf{b}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!-- Trigger the modal with a button -->\n",
    "<button type=\"button\" class=\"btn btn-info btn-lg\" data-toggle=\"modal\" data-target=\"#myModal\">Open Modal</button>\n",
    "\n",
    "<!-- Modal -->\n",
    "<div id=\"myModal\" class=\"modal fade\" role=\"dialog\">\n",
    "  <div class=\"modal-dialog\">\n",
    "\n",
    "    <!-- Modal content-->\n",
    "    <div class=\"modal-content\">\n",
    "      <div class=\"modal-header\">\n",
    "        <button type=\"button\" class=\"close\" data-dismiss=\"modal\">&times;</button>\n",
    "        <h4 class=\"modal-title\">Modal Header</h4>\n",
    "      </div>\n",
    "      <div class=\"modal-body\">\n",
    "        <p>Some text in the modal.</p>\n",
    "      </div>\n",
    "      <div class=\"modal-footer\">\n",
    "        <button type=\"button\" class=\"btn btn-default\" data-dismiss=\"modal\">Close</button>\n",
    "      </div>\n",
    "    </div>\n",
    "\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
